<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
  <title>Xiquan Li</title>
  
  <meta name="author" content="Xiquan Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- <link rel="icon" type="image/png" href="images/seal_icon.png"> -->
</head>


<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">





        <!-- ## -->
        <!-- Bio -->
        <!-- ## -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:20px 0px 20px 20px;width:63%;vertical-align:middle">
              <p style="text-align:center">
              <!-- <name>Xiquan Li 「李希泉」</name> -->
              <name>Xiquan Li 李希泉</name>

              </p>
              <p>I am a dual-degree master's student at <a href="http://en.sjtu.edu.cn"> Shanghai Jiao Tong University (SJTU)</a> and <a href="https://www.telecom-paris.fr"> Télécom Paris</a>, advised by <a href="https://chenxie95.github.io"> Prof. Xie Chen</a> from <a href="https://x-lance.github.io">X-LANCE Lab, SJTU </a>and <a href="https://slimessid.github.io/research/"> Prof. Slim Essid</a> from <a href="https://adasp.telecom-paris.fr">ADASP Group, Télécom Paris</a>
              </a>.
              </p>
              <p>Previously, I received my Bachelor's degree from <a href="http://en.sjtu.edu.cn"> Shanghai Jiao Tong University</a> in 2024. During that summer, I was a research assisstant at <a href="https://www.cuhk.edu.hk/chinese/index.html">The Chinese Univeristy of Hong Kong (CUHK)</a>, working wth <a href="https://qiuqiangkong.github.io">Prof. Qiuqiang Kong</a> at the <a href="http://dsp.ee.cuhk.edu.hk"> DSP Lab, CUHK.</a>
              <!-- </p>
              <p>Email: xiquan.li [AT] telecom-paris.fr </p> -->
              <!-- <p style="text-align: center;">  -->
              
                <br>
                <br>
                Links: &nbsp
                <a href="mailto:xiquan.li@telecom-paris.fr"><i class="fas fa-envelope"></i> Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=cRDM0lIAAAAJ&hl=en"><i class="fas fa-graduation-cap"></i> Google Scholar</a> &nbsp/&nbsp
                <a href="data/CV_Xiquan_Li_EN.pdf"><i class="fas fa-file-pdf"></i> CV</a> &nbsp/&nbsp
                <a href="https://github.com/xiquan-li"><i class="fab fa-github"></i> GitHub</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/xiquan-li-7562ba290/?originalSubdomain=fr"><i class="fab fa-linkedin"></i> LinkedIn</a>
              <!-- </p> -->
            </td>

            <td style="padding:2.5%;width:26%;max-width:26%">
              <a href="images/xiquanli.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/xiquanli.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research Topics</heading>
              <p>
                My primary research interests lie in audio understanding and generation, as well as multimodal large language models. 
                Previously, I have worked on automated audio captioning, large audio language models, contrastive language-audio pre-training and spoken dialogue models.
              <br>
                <!-- Representative papers are <span class="highlight">highlighted</span>. -->
              </p>
            </td>
          </tr>
        </tbody></table>






        <!-- ## -->
        <!-- Publications -->
        <!-- ## -->

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
              <!-- (* equal contribution) -->
            </td>
          </tr>
          <tr>
        </tbody></table>

        <style>
          .publication-item {
            padding-left: 40px; 
            position: relative; 
          }
          .publication-item::before {
            content: "•";
            position: absolute; 
            left: 20px; 
            font-weight: bold; 
            color: black; 
          }
        </style>
        

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <!-- <td style="padding:0px 0px 20px 20px;width:100px;vertical-align:middle">
              <img src="images/urobench.png" alt="urobench" style="width:175px;height:auto;border-radius:5px;">
            </td> -->

            <!--padding: 上 右 下 左-->
            <!-- <td style="padding:0px 0px 20px 40px;width:100%;vertical-align:middle", class="publication-item"> -->

              <td style="padding:0px 0px 20px 20px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2502.17810" id="URO-Bench">
                <papertitle>URO-Bench: A Comprehensive Benchmark for End-to-End Spoken Dialogue Models</papertitle>
              </a>
              <br>
              Ruiqi Yan, <b>Xiquan Li</b>, Wenxi Chen, Zhikang Niu, Chen Yang, Ziyang Ma, Kai Yu, Xie Chen
              <br>
              arXiv, &nbsp;2025&nbsp;
              <br>
              <a href="https://arxiv.org/abs/2502.17810" style="text-decoration: none;">
                Paper
                <!-- <i class="fas fa-file-pdf" style="font-size: 1.2em; color: #d9534f;"></i> Paper -->
              </a>
              &nbsp;/&nbsp;
              <a href="https://github.com/Ruiqi-Yan/URO-Bench" style="text-decoration: none;">
                Code
                <!-- <i class="fab fa-github" style="font-size: 1.2em; color: #333;"></i> Code -->
              </a>
              &nbsp;/&nbsp;
              <a href="https://huggingface.co/datasets/Honggao/URO-Bench" style="text-decoration: none;">
                Dataset
                <!-- <i class="fas fa-database" style="font-size: 1.2em; color: #ffd700;"></i> Dataset -->
              </a>
              <br>

              <font color='gray'>In recent years, with advances in large language models (LLMs), end-to-end spoken dialogue models (SDMs) have made significant strides. Compared to text-based LLMs, the evaluation of SDMs needs to take speech-related aspects into account, such as paralinguistic information and speech quality. However, there is still a lack of comprehensive evaluations for SDMs in speech-to-speech (S2S) scenarios. To address this gap, we propose URO-Bench, an extensive benchmark for SDMs. Notably, URO-Bench is the first S2S benchmark that covers evaluations about multilingualism, multi-round dialogues, and paralinguistics. Our benchmark is divided into two difficulty levels: basic track and pro track, consisting of 16 and 20 datasets respectively, evaluating the model's abilities in Understanding, Reasoning, and Oral conversation. Evaluations on our proposed benchmark reveal that current open-source SDMs perform rather well in daily QA tasks, but lag behind their backbone LLMs in terms of instruction-following ability and also suffer from catastrophic forgetting. Their performance in advanced evaluations of paralinguistic information and audio understanding remains subpar, highlighting the need for further research in this direction. We hope that URO-Bench can effectively facilitate the development of spoken dialogue models by providing a multifaceted evaluation of existing models and helping to track progress in this area. </font>
            </td>
          </tr>
        </tbody></table>

        <!-- <br> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <!-- <td style="padding:0px 0px 20px 40px;width:100px;vertical-align:middle">
              <img src="images/slam-omni.png" alt="slam-omni" style="width:175px;height:auto;border-radius:5px;">
            </td> -->
            <!-- <td style="padding:0px 0px 20px 40px;width:100%;vertical-align:middle", class="publication-item"> -->
              <td style="padding:0px 0px 20px 20px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2412.15649" id="SLAM-Omni">
                <papertitle>SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training</papertitle>
              </a>
              <br>
              Wenxi Chen, Ziyang Ma, Ruiqi Yan, Yuzhe Liang, <b>Xiquan Li</b>, Ruiyang Xu, Zhikang Niu,
              Yanqiao Zhu, Yifan Yang, Zhanxun Liu, Kai Yu, Yuxuan Hu, Jinyu Li, Yan Lu, Shujie Liu, Xie Chen
              <br>
              arXiv, &nbsp;2025&nbsp;
              <br>
              <a href="https://arxiv.org/abs/2412.15649" style="text-decoration: none;">
                Paper
                <!-- <i class="fas fa-file-pdf" style="font-size: 1.2em; color: #d9534f;"></i> Paper -->
              </a>
              &nbsp;/&nbsp;
              <a href="https://github.com/X-LANCE/SLAM-LLM/blob/main/examples/s2s/README.md" style="text-decoration: none;">
                Code
                <!-- <i class="fab fa-github" style="font-size: 1.2em; color: #333;"></i> Code -->
              </a>

              <br>
              <font color="gray">Recent advancements highlight the potential of end-to-end real-time spoken dialogue systems, showcasing their low latency and high quality. In this paper, we introduce SLAM-Omni, a timbre-controllable, end-to-end voice interaction system with single-stage training. SLAM-Omni achieves zero-shot timbre control by modeling spoken language with semantic tokens and decoupling speaker information to a vocoder. By predicting grouped speech semantic tokens at each step, our method significantly reduces the sequence length of audio tokens, accelerating both training and inference. Additionally, we propose historical text prompting to compress dialogue history, facilitating efficient multi-round interactions. Comprehensive evaluations reveal that SLAM-Omni outperforms prior models of similar scale, requiring only 15 hours of training on 4 GPUs with limited data. Notably, it is the first spoken dialogue system to achieve competitive performance with a single-stage training approach, eliminating the need for pre-training on TTS or ASR tasks. Further experiments validate its multilingual and multi-turn dialogue capabilities on larger datasets.</font>
            </td>
          </tr>
        </tbody></table>

        <!-- <br> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <!-- <td style="padding:0px 0px 20px 20px;width:100px;vertical-align:middle">
              <img src="images/drcap.png" alt="drcap" style="width:175px;height:auto;border-radius:5px;">
            </td> -->
            <!-- <td style="padding:0px 0px 20px 40px;width:100%;vertical-align:middle", class="publication-item"> -->
              <td style="padding:0px 0px 20px 20px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2410.09472" id="SLAM-Omni">
                <papertitle>DRCap: Decoding CLAP Latents with Retrieval-Augmented Generation for Zero-shot Audio Captioning
                </papertitle>
              </a>
              <br>
              <b>Xiquan Li</b>, Wenxi Chen, Ziyang Ma, Xuenan Xu, Yuzhe Liang, Zhisheng Zheng, Qiuqiang Kong, Xie Chen
              <br>
              ICASSP, &nbsp;2025&nbsp; <font color='red'>(Oral)</font>
              <br>
              <a href="https://arxiv.org/abs/2410.09472" style="text-decoration: none;">
                Paper
                <!-- <i class="fas fa-file-pdf" style="font-size: 1.2em; color: #d9534f;"></i> Paper -->
              </a>
              &nbsp;/&nbsp;
              <a href="https://github.com/X-LANCE/SLAM-LLM/tree/main/examples/drcap_zeroshot_aac" style="text-decoration: none;">
                Code
                <!-- <i class="fab fa-github" style="font-size: 1.2em; color: #333;"></i> Code -->
              </a>
              <br>
              <font color="gray">While automated audio captioning (AAC) has made notable progress, traditional fully supervised AAC models still face two critical challenges: the need for expensive audio-text pair data for training and performance degradation when transferring across domains. To overcome these limitations, we present DRCap, a data-efficient and flexible zero- shot audio captioning system that requires text-only data for training and can quickly adapt to new domains without additional fine-tuning. DRCap integrates a contrastive language-audio pre-training (CLAP) model and a large language model (LLM) as its backbone. During training, the model predicts the ground-truth caption with a fixed text encoder from CLAP, whereas, during inference, the text encoder is replaced with the audio encoder to generate captions for audio clips in a zero-shot manner. To mitigate the modality gap of the CLAP model, we use both the projection strategy from the encoder side and the retrieval-augmented generation strategy from the decoder side. Specifically, audio embeddings are first projected onto a text embedding support to absorb extensive semantic information within the joint multi-modal space of CLAP. At the same time, similar captions retrieved from a datastore are fed as prompts to instruct the LLM, incorporating external knowledge to take full advantage of its strong generative capability. Conditioned on both the projected CLAP embedding and the retrieved similar captions, the model is able to produce a more accurate and semantically rich textual description. By tailoring the text embedding support and the caption datastore to the target domain, DRCap acquires a robust ability to adapt to new domains in a training-free manner. Experimental results demonstrate that DRCap outperforms all other zero-shot models in in- domain scenarios and achieves state-of-the-art performance in cross- domain scenarios.</font>
            </td>
          </tr>
        </tbody></table>

        <!-- <br> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <!-- <td style="padding:0px 0px 20px 20px;width:100px;vertical-align:middle">
              <img src="images/slam-aac.png" alt="slam-aac" style="width:175px;height:auto;border-radius:5px;">
            </td> -->
            <!-- <td style="padding:0px 0px 20px 40px;width:100%;vertical-align:middle", class="publication-item"> -->
              <td style="padding:0px 0px 20px 20px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/html/2410.09503v1" id="SLAM-AAC">
                <papertitle>SLAM-AAC: Enhancing Audio Captioning with Paraphrasing Augmentation and CLAP-Refine through LLMs
                </papertitle>
              </a>
              <br>
              Wenxi Chen*, Ziyang Ma*, <b>Xiquan Li</b>, Xuenan Xu, Yuzhe Liang, Zhisheng Zheng, Kai Yu, Xie Chen
              <br>
              ICASSP, &nbsp;2025&nbsp;
              <br>
              <a href="https://arxiv.org/html/2410.09503v1" style="text-decoration: none;">
                Paper
                <!-- <i class="fas fa-file-pdf" style="font-size: 1.2em; color: #d9534f;"></i> Paper -->
              </a>
              &nbsp;/&nbsp;
              <a href="https://github.com/X-LANCE/SLAM-LLM/tree/main/examples/slam_aac" style="text-decoration: none;">
                Code
                <!-- <i class="fab fa-github" style="font-size: 1.2em; color: #333;"></i> Code -->
              </a>
              <font color="gray">Automated Audio Captioning (AAC) aims to generate natu- ral textual descriptions for input audio signals. Recent progress in audio pre-trained models and large language models (LLMs) has significantly enhanced audio understanding and textual reasoning capabilities, making improvements in AAC possible. In this paper, we propose SLAM-AAC to further enhance AAC with paraphrasing augmentation and CLAP- Refine through LLMs. Our approach uses the self-supervised EAT model to extract fine-grained audio representations, which are then aligned with textual embeddings via lightweight linear layers. The caption generation LLM is efficiently fine-tuned using the LoRA adapter. Drawing inspiration from the back-translation method in machine translation, we implement paraphrasing augmentation to expand the Clotho dataset during pre-training. This strategy helps alleviate the limitation of scarce audio-text pairs and generates more diverse captions from a small set of audio clips. During inference, we introduce the plug-and-play CLAP-Refine strategy to fully exploit multiple decoding outputs, akin to the n-best rescoring strategy in speech recognition. Using the CLAP model for audio-text similarity calculation, we could select the textual descriptions generated by multiple searching beams that best match the input audio. Experimental results show that SLAM-AAC achieves state- of-the-art performance on Clotho V2 and AudioCaps, surpassing previous mainstream models.</font>
            </td>
          </tr>
        </tbody></table>

        <!-- <br> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <!-- <td style="padding:0px 0px 20px 20px;width:100px;vertical-align:middle">
              <img src="images/emobox.png" alt="emobox" style="width:175px;height:auto;border-radius:5px;"> 
            </td> -->
            <!-- <td style="padding:0px 0px 20px 40px;width:100%;vertical-align:middle", class="publication-item"> -->
              <td style="padding:0px 0px 20px 20px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2406.07162" id="EmoBox">
                <papertitle>EmoBox: Multilingual Multi-corpus Speech Emotion Recognition Toolkit and Benchmark
                </papertitle>
              </a>
              <br>
              Ziyang Ma, Mingjie Chen, Hezhao Zhang, Zhisheng Zheng, Wenxi Chen, <b>Xiquan Li</b>, Jiaxin Ye, Xie Chen, Thomas Hain
              <br>
              Interspeech, &nbsp;2024&nbsp; <font color='red'>(Oral)</font>
              <br>
              <a href="https://arxiv.org/abs/2406.07162" style="text-decoration: none;">
                Paper
                <!-- <i class="fas fa-file-pdf" style="font-size: 1.2em; color: #d9534f;"></i> Paper -->
              </a>
              &nbsp;/&nbsp;
              <a href="https://github.com/emo-box/EmoBox" style="text-decoration: none;">
                Code
                <!-- <i class="fab fa-github" style="font-size: 1.2em; color: #333;"></i> Code -->
              </a>
              &nbsp;/&nbsp;
              <a href="https://emo-box.github.io" style="text-decoration: none;">
                Project Page
                <!-- <i class="fas fa-globe" style="font-size: 1.2em; color: #0073e6;"></i> Project Page -->
              </a>
              <br>
              <font color="gray">Speech emotion recognition (SER) is an important part of human-computer interaction, receiving extensive attention from both industry and academia. However, the current research field of SER has long suffered from the following problems: 1) There are few reasonable and universal splits of the datasets, making comparing different models and methods difficult. 2) No commonly used benchmark covers numerous corpus and languages for researchers to refer to, making reproduction a burden. In this paper, we propose EmoBox, an out-of-the-box multilingual multi-corpus speech emotion recognition toolkit, along with a benchmark for both intra-corpus and cross-corpus settings. For intra-corpus settings, we carefully designed the data partitioning for different datasets. For cross-corpus settings, we employ a foundation SER model, emotion2vec, to mitigate annotation errors and obtain a test set that is fully balanced in speakers and emotions distributions. Based on EmoBox, we present the intra-corpus SER results of 10 pre-trained speech models on 32 emotion datasets with 14 languages, and the cross-corpus SER results on 4 datasets with the fully balanced test sets. To the best of our knowledge, this is the largest SER benchmark, across language scopes and quantity scales. We hope that our toolkit and benchmark can facilitate the research of SER in the community.</font>
            </td>
          </tr>
        </tbody></table>





        <!-- ## -->
        <!-- Education -->
        <!-- ## -->
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Education</heading>
            </td>
          </tr>
          <tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
              <b>Shanghai Jiao Tong University</b>, Shanghai, China
              <br>
              M.E. in Information Engineering • Sep. 2024 &ndash; March. 2027
              </br>
            </td>
            <td style="padding:0px 0px 0px 20px;width:10%;vertical-align:middle"><img src="images/sjtu-logo.png" width="75" height="75"></td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
              <b>Télécom Paris</b>, Palaiseau, France
              <br>
              M.E. in Information Engineering • Sep. 2023 &ndash; Jun. 2026
              </br>
            </td>
            <td style="padding:0px 0px 0px 20px;width:10%;vertical-align:middle"><img src="images/telecom_paristech.jpg" width="75" height=auto></td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
              <b>Shanghai Jiao Tong University</b>, Shanghai, China
              <br>
              B.E. in Information Engineering, Dual degree in French • Sep. 2020 &ndash; Jun. 2024
              </br>
            </td>
            <td style="padding:0px 0px 0px 20px;width:10%;vertical-align:middle"><img src="images/sjtu-logo.png" width="75" height="75"></td>
          </tr>
        </tbody></table>





        <!-- ## -->
        <!-- Experiences -->
        <!-- ## -->

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Experiences</heading>
            </td>
          </tr>
          <tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
              <a href="https://adasp.telecom-paris.fr"> ADASP Group</a>, <b>Télécom Paris, Institut Polytechnique de Paris</b>
              <br>
              Research Intern • Sep. 2024 &ndash; June. 2025
              <br>
              Advisor: <a href="https://slimessid.github.io/research/"> Slim Essid</a>
              </br>
            </td>
            <td style="padding:0px 0px 0px 20px;width:10%;vertical-align:middle"><img src="images/telecom_paristech.jpg" width="75" height=auto></td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
              <a href="http://dsp.ee.cuhk.edu.hk"> DSP Lab</a>, <b>The Chinese University of Hong Kong </b>
              <br>
              Research Assisstant • June. 2024 &ndash; Sep. 2024
              <br>
              Advisor: <a href="https://scholar.google.com/citations?user=B6O3SycAAAAJ&hl=en&oi=ao"> Qiuqiang Kong </a> 
              </br>
            </td>
            <td style="padding:0px 0px 0px 20px;width:10%;vertical-align:middle"><img src="images/CUHK_Logo.png" width="75" height=auto></td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
              <a href="https://x-lance.github.io"> X-LANCE Lab</a>, <b>Shanghai Jiao Tong University</b>
              <br>
              Research Intern • Jan. 2023 &ndash; Now
              <br>
              Advisor: <a href="https://chenxie95.github.io"> Xie Chen</a>
              </br>
            </td>
            <td style="padding:0px 0px 0px 20px;width:10%;vertical-align:middle"><img src="images/sjtu-logo.png" width="75" height="75"></td>
          </tr>
        </tbody></table>



                

        <!-- ## -->
        <!-- Misc. -->
        <!-- ## -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Misc.</heading>
            <p>
              Apart from research, I love skiing, playing soccer and hitting the gym. Check out some of my wonderful ski moments  <a href="ski.html">here</a> :)
              <!-- Check out some of the wonderful skiing moments <a href="ski.html">here</a> :) -->
            </p>


            </td>
          </tr>
        </tbody>

      </table>




        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td style="padding:0px">
                                    <br>
                                    <br>
                                    <div>
                                        <!-- <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=080808&w=350&t=tt&d=Biz007_Pw8FVsAWycLRoKM_5XR_da9ccb8qGNbWVwnk&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080"></script> -->
                                        <!-- <a target="_top" href="http://clustrmaps.com/site/1acpn?utm_source=widget&amp;utm_campaign=widget_ctr" id="clustrmaps-widget-v2" class="clustrmaps-map-control" style="width: 300px;">
 -->                               </div>
                                </td>
                            </tr>
                        </tbody>
        </table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px">
                <p font-size:small;="">
                    <br>
                    <br>
                    </p><div style="float:left;">
                        Updated at March. 2025
                    </div>
                    <div style="float:right;">
                        Thanks <a href="https://jonbarron.info">Jon Barron</a> for this amazing template
                    </div>
                    <br>
                    <br>        
                <p></p>                           
            </td>
          </tr>
        </tbody></table>

      </td>
    </tr>
  </table>
</body>

</html>
